DataKind UK Ethics Book Club Materials - 2019
================
DataKind UK
19/01/2020

All our 2019 DataKind UK Ethics Book Club materials, in one place. For
convenience - to save people having to go to each topic.

## 1\. Impact of Algorithms

### Materials for Book Club

1.  [**Weapons of Math
    Destruction**](https://weaponsofmathdestructionbook.com/). Cathy
    O’Neil, 2016. We kinda had to start with this, right? One of the
    books that got people talking about ethical issues with AI, data
    science etc. NOTE: This is a book that costs money\! If you want to
    buy it that’s great, but if anyone has a copy they can lend out then
    post here. Also check local libraries as a good resource\!

2.  [**The controversial tech used to predict problems before they
    happen**](https://news.sky.com/story/the-controversial-tech-used-to-detect-problems-before-they-happen-11649080).
    Manthorpe, 2019. A report from Sky News (**Warning\! auto-playing
    video**) This is a news report on a longer report on algorithm use
    on UK governments by the Data Justice Lab Cardiff. Read the Sky
    story, watch the news report, go dig out the 144-page pdf of the
    full report - it’s all good :)

3.  [**Fairness and Accountability Design Needs for Algorithmic Support
    in High-Stakes Public Sector
    Decision-Making**](https://arxiv.org/abs/1802.01029) - Veale et al.,
    2018. Journal article based on interviews with data scientists and
    others involved in creating and implementing those algorithmic tools
    in the UK - a chance to dig in to what goes on behind the scenes\!

## 2\. Facial Recognition

### Materials for Book Club

This session we will be discussing **Facial Recognition** - the good,
the bad and the ugly\! No need to read everything - pick a few\! If
you’ve just got time for one, pick the one in yellow.

**Facial recognition is bad?**

  - Facial recognition is the plutonium of AI. by Luke Stark, Published
    in Association for Computer Machinery it argues that facial
    recognition has few legitimate uses
    [article](https://static1.squarespace.com/static/59a34512c534a5fe6721d2b1/t/5cb0bf02eef1a16e422015f8/1555087116086/Facial+Recognition+is+Plutonium+-+Stark.pdf)

**Facial recognition is good?**

  - BBC
    [article](https://www.bbc.co.uk/news/uk-scotland-edinburgh-east-fife-47614890)
    on pigs - Facial recognition tool ‘could help boost pigs’ wellbeing
  - Collateral
    [article](https://www.collater.al/en/facebook-lim-si-ping-installation/)
    on art - Art director Lim Si Ping has created Facebook, the first
    digital book that, based on facial recognition, tells the perfect
    story for you
  - China uses facial recognition technology to cut down on toilet paper
    theft
    [article](https://www.washingtonpost.com/news/morning-mix/wp/2017/03/21/china-uses-facial-recognition-software-to-crack-down-on-toilet-paper-theft/?noredirect=on&utm_term=.cf121bf595a6)

**Should we get ugly? Activism**

For a general background, have a browse of the **Algorithmic Justice
League** [website](https://www.ajlunited.org/). The recent resistance to
Amazon’s use of FR is a case in point. Further reading below:

  - **Joy Buolamwini’s response to Amazon** (Medium
    [post](https://medium.com/@Joy.Buolamwini/response-racial-and-gender-bias-in-amazon-rekognition-commercial-ai-system-for-analyzing-faces-a289222eeced))
    - *Response: Racial and Gender bias in Amazon
    Rekognition — Commercial AI System for Analyzing Faces*
  - **Letter from ‘Concerned researchers’** (Medium
    [post](https://medium.com/@bu64dcjrytwitb8/on-recent-research-auditing-commercial-facial-analysis-technology-19148bda1832))
    - *On Recent Research Auditing Commercial Facial Analysis
    Technology*
  - [**AI Now
    Letter**](https://ainowinstitute.org/dhcr-amici-letter-043019.pdf)
    re: Dangers of using facial recognition for housing.
      - Write up of the letter (The Verge
        [article](https://www.theverge.com/2019/4/3/18291995/amazon-facial-recognition-technology-rekognition-police-ai-researchers-ban-flawed))
        - AI researchers tell Amazon to stop selling ‘flawed’ facial
        recognition to the police

### Further Reading

*Reports *

  - Oxfam
    [report](https://policy-practice.oxfam.org.uk/publications/biometrics-in-the-humanitarian-sector-620454)
    on *Biometrics in the Humanitarian Sector *
  - Short report on *police use of facial recognition* (UK Govt,
    [here](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/781745/Facial_Recognition_Briefing_BFEG_February_2019.pdf))
  - [US
    Government's](https://www.ntia.doc.gov/files/ntia/publications/aclu_an_ethical_framework_for_face_recognition.pdf)*Ethical
    Framework for Facial Recognition*
  - SAFE pledge from the Algorithmic Justice League
    [*https://www.safefacepledge.org/pledge*](https://www.safefacepledge.org/pledge)

*Articles *

  - [Article](https://howwegettonext.com/silicon-valley-thinks-everyone-feels-the-same-six-emotions-38354a0ef3d7):
    *Silicon Valley Thinks Everyone Feels the Same Six Emotions *
  - FT
    [article](https://www.ft.com/content/c2300a60-5a2b-11e9-939a-341f5ada9d40):
    *Taser stun gun maker files facial recognition patents*
  - Non-consensual use of data for facial recognition training (Slate,
    [here](https://slate.com/technology/2019/03/facial-recognition-nist-verification-testing-data-sets-children-immigrants-consent.html))
    - *The Government Is Using the Most Vulnerable People to Test Facial
    Recognition Software*
  - New York Times - *One Month, 500,000 Face Scans: How China Is Using
    A.I. to Profile a Minority*
    ([here](https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html))

## 3\. Fairness in AI

**How do we define fairness?**

  - [21 Fairness definitions](https://fairmlbook.org/tutorial2.html) and
    their politics - Arvind Narayanan, 2018 \[video; 1hr\]

  - DataKind UK’s Giselle Cory wrote a
    [blog](https://medium.com/datakinduk/defining-fairness-1e12586d4b36)
    summarising and reflecing on Narayanan’s work, launched ahead of the
    book club.

**Background**

  - [Machine
    Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
    - the Propublica COMPAS story that is the key reference point in
    talks about algorithmic bias and unfair outcomes \[article\]
  - [Where fairness
    fails](https://static1.squarespace.com/static/5b8ab61f697a983fd6b04c38/t/5cd9934e9b747a265111e80a/1557762900322/Where+fairness+fails+data+algorithms+and+the+limits+of+antidiscrimination+discourse.pdf):
    data, algorithms, and the limits of antidiscrimination discourse
    Anna Lauren Hoffman, 2019 \[academic article\]

**Hands on**

  - [IBM AI
    Fairness 360](https://www.ibm.com/blogs/research/2018/09/ai-fairness-360/)
    tool \[blog + links to interactive tutorials\]

## 4\. Self-Driving Cars

### Materials for Book Club

  - [**Car Wars**](https://this.deakin.edu.au/self-improvement/car-wars)
    **(Cory Doctorow 2016)**
  - Here’s an alternative shorter take from
    [Guardian](https://www.theguardian.com/technology/2015/dec/23/the-problem-with-self-driving-cars-who-controls-the-code)
    by the same author

### Further Reading

For those who want more….

*A primer*

  - And for those who would like it, a primer on self-driving cars:
    **Self-driving car technology explained to non experts**
    ([Medium](https://medium.com/swlh/everything-about-self-driving-cars-explained-for-non-engineers-f73997dcb60c))

*Impact on inequality *

  - [Academic paper](https://arxiv.org/pdf/1902.11097.pdf): Predictive
    Inequity in Object Detection *- systems unable to identify across
    skin colors. *
  - In this work, we investigate whether state-of-the-art object
    detection systems have equitable predictive performance on
    pedestrians with different skin tones. This work is motivated by
    many recent examples of ML and vision systems displaying higher
    error rates for certain demographic groups than others. We annotate
    an existing large scale dataset which contains pedestrians, BDD100K,
    with Fitzpatrick skin tones in ranges \[1-3\] or \[4-6\]. We then
    provide an in depth comparative analysis of performance between
    these two skin tone groupings, finding that neither time of day nor
    occlusion explain this behavior, suggesting\_ **this disparity is
    not merely the result of pedestrians in the 4-6 range appearing in
    more difficult scenes for detection** *. We investigate to what
    extent time of day, occlusion, and reweighting the supervised loss
    during training affect this predictive bias.*
  - See also news article on this paper, A new study finds a potential
    risk with self-driving cars: failure to detect dark-skinned
    pedestrians
    ([*Vox*](https://www.vox.com/future-perfect/2019/3/5/18251924/self-driving-car-racial-bias-study-autonomous-vehicle-dark-skin)\_,
    March 2019)
  - Research paper: Self-Driving Cars: The Impact on People with
    Disabilities ([Ruderman
    Foundation](https://rudermanfoundation.org/white_papers/self-driving-cars-the-impact-on-people-with-disabilities/),
    January 2017)

*Questions of morality….*

  - Interactive scenarios: The Moral Machine
    ([MIT](http://moralmachine.mit.edu/))
      - Academic paper: The Moral Machine Experiment
        ([Nature](https://www.researchgate.net/publication/328491510_The_Moral_Machine_Experiment),
        November 2018) - \_this article lays some interesting
        foundations on the absence of universal moral rules (differing
        by country).
      - See also the review of the paper, providing some more context:\_
        Self-driving car dilemmas reveal that moral choices are not
        universal
        ([Nature](https://www.nature.com/articles/d41586-018-07135-0#ref-CR1),
        October 2018)
      - News article: Will your driverless car be willing to kill you to
        save the lives of others?
        ([Guardian](https://www.theguardian.com/science/2016/jun/23/will-your-driverless-car-be-willing-to-kill-you-to-save-the-lives-of-others),
        June 2016) *People like utilitarian view in theory, but don’t
        want to buy a car programmed that way*

*Industry responses*

  - News article: 11 companies propose guiding principles for
    self-driving vehicles ([Venture
    Beat](https://venturebeat.com/2019/07/02/self-driving-car-report-safety-first-for-automated-driving/),
    July 2019)
  - Comment piece: Bob Lutz: Kiss the good times goodbye ([Automotive
    News](https://www.autonews.com/article/20171105/INDUSTRY_REDESIGNED/171109944/bob-lutz-kiss-the-good-times-goodbye),
    November 2017) - *Bob Lutz (ex-head of General Motors) gives a
    summary of where he thinks things might be heading *

*Technical challenges and solutions *

  - Mobileye CEO Amnon Shashua's recent lecture at MIT on the challenges
    of reaching full autonomy
    ([video](https://cbmm.mit.edu/video/successes-and-challenges-modern-artificial-intelligence))
  - The challenge of adversarial attacks
    ([article](https://www.technologyreview.com/s/613170/emtech-digital-dawn-song-adversarial-machine-learning/))
  - Mobileye’s 2018 [paper](https://arxiv.org/pdf/1708.06374.pdf) laying
    out a reasonable model of self-driving behavior

## 5\. AI and Gender

### Materials for Book Club

**Book** : [Invisible Women: Data Bias in a World Designed for
Men](https://www.goodreads.com/book/show/41104077-invisible-women) by
Caroline Criado Pérez

**News article** : “[The deadly truth about a world built for men – from
stab vests to car
crashes](https://www.theguardian.com/lifeandstyle/2019/feb/23/truth-world-built-for-men-car-crashes)”
by Caroline Criado Pérez

*Crash-test dummies based on the ‘average’ male are just one example of
design that forgets about women – and puts lives at risk*

**Journal article** : [The Misgendering
Machines](https://ironholds.org/resources/papers/agr_paper.pdf):
Trans/HCI Implications of Automatic Gender Recognition by Os Keyes

*Automatic Gender Recognition (AGR) is a subfield of facial recognition
that aims to algorithmically identify the gender of individuals from
photographs or videos. In wider society, the technology has proposed
applications in physical access control, data analytics and advertising.
Within academia, it is already used in the field of Human-Computer
Interaction (HCI) to analyse social media usage…I show that AGR
consistently operationalises gender in a trans-exclusive way, and
consequently carries disproportionate risk for trans people subject to
it.*

**Report** : I'd blush if I could: closing gender divides in digital
skills through education by Unesco - *specifically we're looking
at*[Think Piece 2 - The Rise of Gendered AI and its Troubling
Repercussions](https://unesdoc.unesco.org/ark:/48223/pf0000367416.page=85),
pages 85-146

**News article** : [“Digital assistants like Siri and Alexa entrench
gender biases, says
UN”](https://www.theguardian.com/technology/2019/may/22/digital-voice-assistants-siri-alexa-gender-biases-unesco-says)
by Kevin Rawlinson

*Assigning female genders to digital assistants such as Apple's Siri and
Amazon's Alexa is helping entrench harmful gender biases, according to a
UN agency.*

### Further Reading

  - Gender and AI in the workplace
    [link](https://ainowinstitute.org/discriminatingsystems.pdf)

## 6\. AI & Financial Inclusion

### Materials for Book Club

Our next data science ethics bookclub is on *AI and bad credit: the
impact of automation on financial inclusion.* **You are welcome to pick
from this reading list, depending on your interest and the time you
have:**

  - **Blog:** ‘We Didn't Explain the Black Box – We Replaced it with an
    Interpretable Model’, about the FICO explainable credit score
    competition winner
    [here](https://community.fico.com/s/blog-post/a5Q2E0000001czyUAA/fico1670)
  - **Academic article** on fairness in credit risk evaluation,
    ‘Context-conscious fairness in using machine learning to make
    decisions’
    [here](https://sigai.acm.org/static/aimatters/5-2/AIMatters-5-2-07-Lee.pdf)
  - **Government paper** : The Centre for Data Ethics and Innovation
    look at AI and Personal Insurance
    [here](https://www.gov.uk/government/publications/cdei-publishes-its-first-series-of-three-snapshot-papers-ethical-issues-in-ai/snapshot-paper-ai-and-personal-insurance)
  - **News article** on how companies can use data with low levels of
    regulation, ‘The new lending game, post-demonetisation’
    [here](https://tech.economictimes.indiatimes.com/news/technology/the-new-lending-game-post-demonetisation/56367457)
  - **Academic article** on discrimination in consumer lending
    [here](https://faculty.haas.berkeley.edu/morse/research/papers/discrim.pdf)

### Further Reading

  - <https://www.fca.org.uk/publication/research/price_discrimination_in_financial_services.pdf>

  - <https://www.fca.org.uk/publication/feedback/fs19-04.pdf>

## 7\. AI and Race

### Materials for Book Club

Our next data science ethics bookclub is on *AI and Race.* **You are
welcome to pick from this reading list, depending on your interest and
the time you have:**

**Main read Book:** Ruha Benjamin, Race After Technology

**Journal Article:** Sebastian Benthall & Bruce D. Haynes. **Racial
Categories in machine learning** -
<https://arxiv.org/pdf/1811.11668.pdf>

**Quick reads** Jessie Daniels, Mutale Nkonde, Darakhshan Mir
**Advancing Racial Literacy in Tech- Why ethics, diversity in hiring and
implicit bias trainings aren't enough.**
<https://datasociety.net/wp-content/uploads/2019/05/Racial\_Literacy\_Tech\_Final\_0522.pdf>

Karen Hao, **This is how AI bias really happens—and why it's so hard to
fix. MIT Technology Review**
<https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/>

**Short watch** **Poetry:** Poem performed by Joy Buolamwini, AI,
**Ain't I A Woman?** <https://www.notflawless.ai/> **Talk:** Safiya
Noble, author of Algorithms of Oppression provides a short discussion of
her book <https://youtu.be/6KLTpoTpkXo> **Comedy sketch:** Full Frontal
with Samantha Bee. Correspondant Sasheer Zamata discusses bias
<https://youtu.be/AxpWvMrPqVs>
